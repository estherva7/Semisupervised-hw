clear all; close all;
load mnist_uint8.mat

train_x = double(train_x)/255;
test_x  = double(test_x)/255;
train_y = double(train_y);
test_y  = double(test_y);

Xtrain = test_x; ytrain = vec2ind(test_y');
Xtest = train_x; ytest = vec2ind(train_y');

no_dims = 100;

layers = [784 100]; %946   199   101   100

disp(['Network size: ' num2str(layers)]);

lambda = 0;
noise = 0.3;
max_iter = 1;

% Pretrain model using stacked denoising auto-encoders

no_layers = length(layers);
model = cell(2 * no_layers, 1);

for i=1:no_layers
    [network, mappedX] = train_autoencoder(Xtrain, layers(i), noise, max_iter);
    model{i}.W        = network{1}.W;
    model{i}.bias_upW = network{1}.bias_upW;
end

for i=1:no_layers
    model{no_layers + i}.W        = model{no_layers - i + 1}.W';
    if i ~= no_layers
        model{no_layers + i}.bias_upW = model{no_layers - i}.bias_upW;
    else
        model{no_layers + i}.bias_upW = zeros(1, size(Xtrain, 2));
    end
end

clear network mappedX
 
% Compute mean squared error of initial model predictions
reconX = run_data_through_autoenc(model, Xtrain);
disp(['MSE of initial model: ' num2str(mean((reconX(:) - Xtrain(:)) .^ 2))]);

% Finetune model using gradient descent
noise = 0.2;
max_iter = 5;
lambda = 0.1;

model = backprop(model, Xtrain, Xtrain, max_iter, noise, lambda);

 

% Compute mean squared error of final model predictions

[reconX, mappedXtrain] = run_data_through_autoenc(model, Xtrain);

disp(['MSE of final model: ' num2str(mean((reconX(:) - Xtrain(:)) .^ 2))]);

[reconX, mappedXtest] = run_data_through_autoenc(model, Xtest);

disp(['MSE of final model: ' num2str(mean((reconX(:) - Xtest(:)) .^ 2))]);

 

%%

modelSVM = knnFit(mappedXtrain, ytrain, 20);
[yhat] = knnPredict(modelSVM, mappedXtest);      

%%  ex1 train a 100 hidden unit SDAE and use it to initialize a FFNN

%  Setup and train a stacked denoising autoencoder (SDAE)

rand('state',0)

sae = saesetup([784 100]);

sae.ae{1}.activation_function       = 'sigm';

sae.ae{1}.learningRate              = 1;

sae.ae{1}.inputZeroMaskedFraction   = 0.2;

opts.numepochs =   10;

opts.batchsize = 100;

sae = saetrain(sae, Xtrain, opts);

visualize(sae.ae{1}.W{1}(:,2:end)')

 

% Use the SDAE to initialize a FFNN

nn = nnsetup([784 100 10]);

nn.activation_function              = 'sigm';

nn.learningRate                     = 1;

nn.W{1} = sae.ae{1}.W{1};

 

% Train the FFNN

opts.numepochs =   20;

opts.batchsize = 100;

nn = nntrain(nn, Xtrain, test_y, opts);

[er, bad] = nntest(nn, Xtest, train_y);

er

assert(er < 0.16, 'Too big error');

% Use another classifier instead
Xtrain_feat = Xtrain*sae.ae{1,1}.W{1,2};
Xtest_feat = Xtest*sae.ae{1,1}.W{1,2};

modelSVM = knnFit(Xtrain_feat, ytrain, 20);
[yhat] = knnPredict(modelSVM, mappedXtest);  



